{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab IS&A\n",
    "## Bagattin Enrico - Alessandro Doretto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Modules\n",
    "from utilities import *\n",
    "from dataPreparation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "yearsForFeatures = [2016, 2017, 2018, 2019, 2020]\n",
    "paths = []\n",
    "for y in years:\n",
    "    paths.append('matches/' + str(y) + '.xlsx') \n",
    "availablePaths = list(glob.glob(\"matches/20*.xlsx\"))\n",
    "matches = [pd.read_excel(path) for path in paths]\n",
    "yearZeroForFeatures = pd.read_excel('matches/' + str(years[0]-1) + '.xlsx')\n",
    "# TODO: Load matches based on number of past years choosen\n",
    "df = pd.concat(matches, ignore_index=True, sort=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.describe(include='all', percentiles=[]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cleaning and preparing data\n",
    "\n",
    "## Remove Winner/Loser reference\n",
    "All the column with Winner/Loser reference will be substituted by Player0/Player1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = removeWinnerLoserReference(df)\n",
    "yearZeroForFeatures = removeWinnerLoserReference(yearZeroForFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling null:\n",
    "* Rank: take the max rank plus one\n",
    "* Pts: set default zero\t\n",
    "* Avg odd: take mode of matches with same (or similar) players rank\n",
    "* B365, PS: fill with avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rankDefault = max(df['Rank0'].max(), df['Rank1'].max())+1\n",
    "df.fillna({'Rank0': rankDefault, 'Rank1': rankDefault, 'Pts0': 0, 'Pts1': 0}, inplace=True)\n",
    "\n",
    "nullOddsDf = df[df[['B3650', 'B3651', 'PS0', 'PS1', 'Avg0', 'Avg1']].isna().any(axis=1)]\n",
    "for index, row in nullOddsDf.iterrows():\n",
    "    if pd.isnull(row['Avg0']) or pd.isnull(row['Avg1']):\n",
    "        Avg0, Avg1 = findOddsForRow(row, df.dropna(subset=['Avg0', 'Avg1']))\n",
    "        df.at[index, 'Avg0'] = row['Avg0'] = Avg0\n",
    "        df.at[index, 'Avg1'] = row['Avg1'] = Avg1\n",
    "    if pd.isnull(row['B3650']):\n",
    "        df.at[index, 'B3650'] = row['Avg0']\n",
    "    if pd.isnull(row['B3651']):\n",
    "        df.at[index, 'B3651'] = row['Avg1']\n",
    "    if pd.isnull(row['PS0']):\n",
    "        df.at[index, 'PS0'] = row['Avg0']\n",
    "    if pd.isnull(row['PS1']):\n",
    "        df.at[index, 'PS1'] = row['Avg1']\n",
    "\n",
    "df.dropna(subset=['Avg0', 'Avg1'], inplace=True) # Drop rows that hasn't similar rank matches\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Round ????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X['Round'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# you might change this according to a notion of weight\n",
    "# X['Round'] = X['Round'].map ({  '1st Round'    : 1, \n",
    "#                                 '2nd Round'    : 2, \n",
    "#                                 '3rd Round'    : 4,\n",
    "#                                 '4th Round'    : 8,\n",
    "#                                 'Quarterfinals': 16,\n",
    "#                                 'Round Robin'  : 32,\n",
    "#                                 'Semifinals'   : 32,\n",
    "#                                 'The Final'    : 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New features\n",
    "* [Elo rating](https://en.wikipedia.org/wiki/Elo_rating_system): a method for calculating the relative skill levels of players in zero-sum games\n",
    "* Number of matches played during the last year\n",
    "* Percentage of matches won during the last year\n",
    "* Injuries: number matches in witch the player retired or walkover in the past year \n",
    "* Winning streak: current sequence of won games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = addEloRatingFeature(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = addMatchesPlayedAndWonFeatures(X, yearZeroForFeatures, yearsForFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = addInjuriesAndWinningStreakFeatures(X, yearZeroForFeatures, yearsForFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X.to_csv('generated/beforeDuplication.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row duplication\n",
    "To use both match outcomes for our prediction models we will duplicate each row. We can do it by switching all the player features for each duplicated row and adding a Winner column for the match result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "duplication = X.copy()\n",
    "duplication.columns = ['Date', 'Location', 'Tournament', 'Series', 'Court', 'Surface', 'Round',\n",
    "       'Player1', 'Player0', 'Rank1', 'Rank0', 'Pts1', 'Pts0', 'Comment',\n",
    "       'B3651', 'B3650', 'PS1', 'PS0', 'Avg1', 'Avg0', 'EloRating1',\n",
    "       'EloRating0', 'MatchesPlayed1', 'MatchesPlayed0', 'MatchesWon1',\n",
    "       'MatchesWon0', 'Injuries1', 'Injuries0', 'WinningStreak1',\n",
    "       'WinningStreak0']\n",
    "\n",
    "# Add the winner column\n",
    "X = X.assign(Winner=np.zeros(X.shape[0])) # Player 0 always win\n",
    "duplication = duplication.assign(Winner=np.ones(X.shape[0])) # Player 1 always win\n",
    "\n",
    "X = pd.concat([X, duplication])\n",
    "X.reset_index(inplace=True)\n",
    "X.sort_values(by='index', inplace=True)\n",
    "X.drop(columns=['Date', 'Comment', 'index'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "* Location\n",
    "* Tournament\n",
    "* Series\n",
    "* Court\n",
    "* Surface\n",
    "* Round\n",
    "* Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)\n",
    "print('Total number of columns:', len(X.columns))\n",
    "\n",
    "X.to_csv('generated/finalDataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset subdivision: Train, Validation, Test\n",
    "\n",
    "Train 60%, Validation 20%, Test 20% (taking as test the last part of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv('generated/finalDataset.csv')\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = X.Winner.values\n",
    "X.drop(columns='Winner', inplace=True)\n",
    "\n",
    "test_size = len(X)//5\n",
    "X_test     = X[-test_size:]\n",
    "y_test     = y[-test_size:]\n",
    "X_train_80 = X[:-test_size]\n",
    "y_train_80 = y[:-test_size]\n",
    "\n",
    "# Random split for training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_80, y_train_80, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction models\n",
    "We start by calculating how much powerful are the bookmakers' alghoritms, then we create and tune ours, let's see the results\n",
    "## Baseline\n",
    "Our first goal is to beat the average bookmaker accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Player 1 wins if the odd is smaller than player 0\n",
    "baseline = X_test['Avg1'] < X_test['Avg0']\n",
    "baseline = baseline.astype(int)\n",
    "baseline_test_acc = accuracy_score(y_true=y_test, y_pred=baseline)\n",
    "print (\"Test Accuracy: {:.2f}\".format(baseline_test_acc*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest-Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaled_train = scaler.transform(X_train)\n",
    "scaled_valid = scaler.transform(X_valid)\n",
    "scaled_train_80 = scaler.transform(X_train_80)\n",
    "scaled_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: kNN is very slow with this dataset\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for k in range(1,16):\n",
    "    kNN = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    kNN.fit(scaled_train, y_train)\n",
    "    y_pred = kNN.predict(scaled_valid)\n",
    "    valid_acc = accuracy_score(y_true=y_valid, y_pred=y_pred)\n",
    "    print (\"k: {:2d} | Validation Accuracy: {:.3f}\".format(k, valid_acc))\n",
    "    accuracies += [[valid_acc, k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the resulting accuracies for a faster running process purpose\n",
    "\n",
    "accuracies =  [[0.5272562083585706, 1],\n",
    "                [0.5399757722592369, 2],\n",
    "                [0.5590551181102362, 3],\n",
    "                [0.5632949727437916, 4],\n",
    "                [0.5793458509993943, 5],\n",
    "                [0.5838885523924894, 6],\n",
    "                [0.5990308903694731, 7],\n",
    "                [0.5896426408237432, 8],\n",
    "                [0.6005451241671714, 9],\n",
    "                [0.595396729254997, 10],\n",
    "                [0.6053906723198061, 11],\n",
    "                [0.5993337371290127, 12],\n",
    "                [0.6035735917625682, 13],\n",
    "                [0.5999394306480921, 14],\n",
    "                [0.6078134463961236, 15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy, best_k = max(accuracies)\n",
    "print ( \"Best K\", best_k )\n",
    "\n",
    "# here we are using both training and validation,\n",
    "# to exploit the most data\n",
    "\n",
    "kNN = neighbors.KNeighborsClassifier(n_neighbors=best_k)\n",
    "kNN.fit(scaled_train_80, y_train_80)\n",
    "\n",
    "# Finally evaluate on test\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=kNN.predict(scaled_test))\n",
    "print (\"Test Accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# train and predict\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# compute Accuracy\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=gnb.predict(X_train))\n",
    "valid_acc = accuracy_score(y_true=y_valid, y_pred=gnb.predict(X_valid))\n",
    "print (\"Train Accuracy: {:.3f} - Validation Accuracy: {:.3f}\".format(train_acc, valid_acc))\n",
    "\n",
    "gnb.fit(X_train_80,y_train_80)\n",
    "\n",
    "# Finally evaluate on test\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=gnb.predict(X_test))\n",
    "print (\"Test Accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for max_leaves in range(5, 101, 5):\n",
    "    # train and predict\n",
    "    dt = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaves)\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    # compute Accuracy\n",
    "    train_acc = accuracy_score(y_true=y_train, y_pred=dt.predict(X_train))\n",
    "    valid_acc = accuracy_score(y_true=y_valid, y_pred=dt.predict(X_valid))\n",
    "    print (\"Leaves: {:2d} - Train Accuracy: {:.3f} - Validation Accuracy: {:.3f}\".format(\n",
    "        max_leaves,  train_acc, valid_acc) )\n",
    "    \n",
    "    accuracies += [ [valid_acc, max_leaves] ]\n",
    "\n",
    "best_accuracy, best_max_leaves = max(accuracies)\n",
    "print ( \"Best Max Leaves\", best_max_leaves )\n",
    "\n",
    "# here we are using both training and validation,\n",
    "# to exploit the most data\n",
    "dt = tree.DecisionTreeClassifier(max_leaf_nodes=best_max_leaves)\n",
    "dt.fit(X_train_80,y_train_80)\n",
    "\n",
    "# Finally evaluate on test\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=dt.predict(X_test))\n",
    "print (\"Test Accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for estimators in range(1, 51):\n",
    "    # train a decision tree classifier\n",
    "    rf = RandomForestClassifier(n_estimators=estimators)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # compute Accuracy\n",
    "    train_acc = accuracy_score(y_true=y_train, y_pred=rf.predict(X_train))\n",
    "    valid_acc = accuracy_score(y_true=y_valid, y_pred=rf.predict(X_valid))\n",
    "    print (\"Estimators: {:2d} - Train Accuracy: {:.3f} - Validation Accuracy: {:.3f}\".format(\n",
    "        estimators,  train_acc, valid_acc) )\n",
    "    \n",
    "    accuracies += [ [valid_acc, estimators] ]\n",
    "\n",
    "best_accuracy, best_estimators = max(accuracies)\n",
    "print ( \"Best Max Leaves\", best_estimators )\n",
    "\n",
    "# here we are using both training and validation,\n",
    "# to exploit the most data\n",
    "rf = tree.DecisionTreeClassifier(max_leaf_nodes=best_estimators)\n",
    "rf.fit(X_train_80,y_train_80)\n",
    "\n",
    "# Finally evaluate on test\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=rf.predict(X_test))\n",
    "print (\"Test Accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Python)",
   "language": "python",
   "name": "pycharm-fa669f56"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
